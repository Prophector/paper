
\section{Information Retrieval}

To visualize the time series to the user and create forecasts, we need to source the data first.

\subsection{Sources}

Daily COVID-19 data is fortunately quite easy to find in machine readable formats. Many governments publish their data daily, but the formats differ for each. We sourced the data of the Federal Office of Public Health\footnote{Link to FOPH API: \href{https://www.covid19.admin.ch/api/data/context}{https://www.covid19.admin.ch/api/data/context}} (FOPH) of the Swiss Confederation. This data contains country wide but also region-specific time series data with raw and aggregated metrics. We also looked at worldwide data from a single source and found Our World in Data\footnote{Link to OWID data: \href{https://ourworldindata.org/coronavirus-source-data}{https://ourworldindata.org/coronavirus-source-data}} (OWID) which publishes reliably daily data from almost every country. OWID has their own data pipelines sourcing their data from official sources to create one place to have all the data available in a coherent format. The FOPH later started publishing worldwide data as well, whereas they sourced it from OWID. The FOPH also uses OWID data to decide which country should be put on a risk list. So, since OWID seems to be a legit data source and even the FOPH trusts them, we decided to have OWID as our main data source but leave the architecture open for other sources to be integrated into the platform. In fact, the FOPH data is still sourced and present in the time series database, but not made available to the end user.

OWID is a good aggregator, as they do the heavy lifting for someone that wants easy access to raw data. For example, on November 30, OWID transitioned from the European Center for Disease Prevention and Control (ECDC) to Johns Hopkins University as our source for confirmed cases and deaths. This followed the ECDCâ€™s announcement that they were switching from daily to weekly updates.

In contrast to OWID, the FOPH source also has hospitalization numbers, whereas OWID does not. We decided to not display hospitalization numbers for now, since they would only be available for Switzerland. And that would complicate the design of an easy-to-use user interface.

We found out that some countries report invalid data, such as Brazil and Tunisia, which report more COVID-19 cases than tests. It is not possible to have more cases with fewer tests. This obviously skews our map visualization for test positivity as we have positivity rates above 100\%.

\subsection{Data Processing}

We implement a scraper and a processor function for each source we want to integrate into our system. The scraper function downloads the files from the source by a tweaked schedule and places them in an AWS S3 bucket for easy retrieval. These raw files also serve as a backup to fully restore the time series database in case of a data loss by processing the data again. We can tune the schedule to download the data according to the sources' publishing schedule. For example, FOPH data is usually released between 13:00 and 14:00 CET/CEST, whereas OWID data is updated continuously.

The processor knows how to read the raw data, creates or updates the country entities with population numbers, if they don't exist yet, and populates the time series database by country and feature (i.e. cases, tests, deaths, vaccinations). It has to do a small amount of data correcting, as for some countries (e.g., tests in Germany) some data is not continuous, whereas it should be a strictly monotone series reporting the total for each day. In case we get a zero but had a larger value before that day, we fill it with the previous value. This error in the data happens, because OWID cannot infer the numbers in between publishings by the countries if they don't publish daily data. For our purposes this is fine, since we will show the daily data, which in this case will just have a spike once a week and the rest of the week it is zero in case somebody looks at the data without any smoothing.

The time series data is then stored in a central PostgreSQL database to make updating and querying the data straight forward. The choice of database does not matter much, since we are not dealing with a large amount of data. Our data is daily, per country and per series. The row count is below 100,000 and should never exceed 1,000,000. This amount can be handled quite easily by almost any database system. Other perfectly as good options would be relational database options such as MySQL, Microsoft SQL Server, or Oracle. NOSQL database options such as MongoDB would also work. We chose PostgreSQL because we were familiar with and because it can be run in a Docker container locally as well "as a service" in the public cloud.

\subsection{Serverless Functions}

The scraping and processing of COVID-19 data only has to be done once or a few times per day. Running a special server that has 100\% uptime handling that, is not cost effective. We decided to use serverless functions by AWS Lambda\footnote{\href{https://aws.amazon.com/lambda}{https://aws.amazon.com/lambda}}. These functions can be triggered by a schedule or by other events and only occur any cost when they are invoked, since they run on shared infrastructure ran by AWS. AWS has a free tier\footnote{\href{https://aws.amazon.com/free}{https://aws.amazon.com/free}} and offers 1 million lambda invocations per month free forever. This means, our information retrieval tool is free, since we do not exceed this limit by some margin.

\subsection{Database Migrations}

Our central database running with PostgreSQL has a schema. This schema needs to be managed and updated once new features are deployed. It is generally a good idea to have automated migrations that are executed by the system automatically by figuring out the outstanding schema migrations and applying them rather than a developer running SQL statements by hand. For that we also leveraged serverless functions and implemented a simple migration handler that updates the central database to a new schema. We decided to do this here, because the scraper and processors are responsible for the sourcing of data, and thus this is the place to also do database migrations. If we do the migrations driven by the web application for example, we will introduce a not straightforward flow of deployments, by first having to deploy the web application, that triggers a migration and only then redeploying the functions sourcing the data.
